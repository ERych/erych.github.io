<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Elizabeth's Portfolio, Experiments in Data Sciecne">


        <title>Predicting Interest in Manhattan Rental Listings // Elizabeth's Portfolio // Experiments in Data Sciecne</title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="./theme/css/pure.css">
    <link rel="stylesheet" href="./theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
    <div class="pure-g-r" id="layout">
        <div class="sidebar pure-u">
            <div class="cover-img" style="background-image: url('https://static.pexels.com/photos/205324/pexels-photo-205324.jpeg')">
                <div class="cover-body">
                    <header class="header">
                        <hgroup>
                            <h1 class="brand-main"><a href=".">Elizabeth's Portfolio</a></h1>
                            <p class="tagline">Experiments in Data Sciecne</p>
                                <p class="links"><a href="/category/projects.html">View Work</a></p>
                                <p class="social">
                                    <a href="https://github.com/erych">
                                        <i class="fa fa-github fa-3x"></i>
                                    </a>
                                    <a href="https://www.linkedin.com/in/elizabeth-rychlinski-54a50a49/">
                                        <i class="fa fa-linkedin-square fa-3x"></i>
                                    </a>
                                </p>
                        </hgroup>
                    </header>
                </div>
            </div>
        </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Predicting Interest in Manhattan Rental Listings</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="./tag/machine-learning.html">machine learning</a>
                                <a class="post-category" href="./tag/python.html">python</a>
                                <a class="post-category" href="./tag/fintech.html">fintech</a>
                        </p>
                </header>
            </section>
            <h4>This project was completed for George Washington University, Machine Learning I, with Lee Eyler, Jacob McKay, and Mikko He</h4>
<h2>Background</h2>
<p>For the DATS6202 Machine Learning I project, our Finance and Economics team participated in the Two Sigma Connect: Rental Listing Inquiries competition on Kaggle. The competition is focused on accurately predicting the demand for rental properties in the New York City area using data provided by Rent Hop. Two Sigma posed the question: How much interest will a new rental listing on renthop receive in New York City? To solve this problem, we created a multi-class prediction model that accurately assigns probabilities to the demand level classes for a particular rental listing.</p>
<p>The prediction model is important because it provides a foundation for:
1. Exploring the types of apartment features that are most indicative of demand
2. Matching availability and demand expectations between landlords/brokers and renters
3. Identifying irregular patterns between listing information and demand</p>
<h2>Data Source</h2>
<p>The Kaggle competition provides a dataset directly from Rent Hop. It provides a comprehensive set of 14 independent attributes that cover all of the key determinants of demand, such as physical location, price, bedroom/bathroom counts, and so forth. It provided a sufficient amount of 124011 observations to validate that the sample size accurately represents the true population of apartments.</p>
<p>The dataset did not require extensive cleansing, thus, we focused on the data preprocessing on feature engineering rather than missing data and outliers. This included the creation of 14 variables additional variables through the following engineering:</p>
<ul>
<li>Date time (year, month, day, hour, day of week)</li>
<li>Number of photos (photo_count)</li>
<li>Price per bathroom and bedroom (price_per_bed; price_per_bedbath)</li>
<li>If listing has an elevator (Elevator)</li>
<li>If listing has doorman (Doorman)</li>
<li>If listing has hardwood floors (Hardwood)</li>
<li>If listing has laundry (Laundry)</li>
<li>If listing has a dishwasher (Dishwasher)</li>
<li>Number of features listed in the “features” column</li>
</ul>
<h2>Data Deficiencies</h2>
<p>There is missing information from the dataset that could help improve this model. The primarily category of excluded data is location-related data; our dataset includes longitude and latitude only. Due to request restrictions from the Google Maps API, we could not use the longitude and latitude variables to pull related data on street address, city, state, or zipcode features. This information would allow us to categorize our geographically data, particularly by neighborhood. The location of a property is likely a high predictor of interest in the listing, and this information would assist us build a better model.</p>
<p>Other valuable information that would be useful but was not included in the data set includes the “walk score,” a metric provided by walkscore.com to assess what is nearby to the property; school information, such as the quality of schools nearby; or, whether a broker fee is required. Fortunately, we were able to ascertain some of this information by Natural Language Processing of the features column.</p>
<h2>Feature Importance</h2>
<p>We set out to determine feature importance using:
1. Random Forest
2. Principal Component Analysis
3. Linear Discriminant Analysis.</p>
<h3>Random Forest</h3>
<p>The Random Forest classifier provided the following results, estimating that about half of the available features are important in regards to decreasing node impurity.  The most important predictors are related to location, price, and time.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># select variables; leaving out categorical data for now</span>
<span class="n">X_train_rf_features</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">26</span><span class="p">,</span><span class="mi">27</span><span class="p">]]</span>
<span class="n">y_train_rf_features</span> <span class="o">=</span> <span class="n">y_train</span>
<span class="n">X_test_rf_features</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">26</span><span class="p">,</span><span class="mi">27</span><span class="p">]]</span>
<span class="n">feature_labels</span> <span class="o">=</span> <span class="n">full_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">26</span><span class="p">,</span><span class="mi">27</span><span class="p">,</span><span class="mi">28</span><span class="p">]]</span>

<span class="c1"># set features for RF</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># fit the model</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_rf_features</span><span class="p">,</span><span class="n">y_train_rf_features</span><span class="p">)</span>

<span class="c1"># obtain feature importances</span>
<span class="n">feat_importance</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">feature_importances_</span>

<span class="c1"># create indices</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">feat_importance</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># print variables and standardized importance score</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_train_rf_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%2d</span><span class="s2">) </span><span class="si">%-*s</span><span class="s2"> </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">feature_labels</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]],</span> <span class="n">feat_importance</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]]))</span>
</pre></div>


<p>Results:
<img alt="alt text" src="https://github.com/ERych/erych.github.io/blob/master/images/FeatureImportance.png" title="Random Forest Results"></p>
<h3>Principal Component Analysis and Linear Discriminant Analysis</h3>
<p>The Principal Component Analysis (unsupervised dimensionality reduction) and Linear Discriminant Analysis (supervised feature extraction) show that projecting the existing variables onto a smaller subspace can still be highly representative of the original data set. Eight principal components account for ~80% of explained variance, while two linear discriminants account for ~100% of explained variance.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c1"># only using numeric and categorical data w/ dummy variables</span>
<span class="n">X_train_for_pca</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">26</span><span class="p">,</span><span class="mi">27</span><span class="p">]]</span>

<span class="c1"># standardize the data</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">stdsc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_for_pca_std</span> <span class="o">=</span> <span class="n">stdsc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_for_pca</span><span class="p">)</span>

<span class="c1"># fitting pca to the data</span>
<span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_for_pca_std</span><span class="p">)</span>

<span class="c1"># obtain feature importances</span>
<span class="n">pca_var_explained</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="n">feature_labels</span> <span class="o">=</span> <span class="n">full_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">26</span><span class="p">,</span><span class="mi">27</span><span class="p">,</span><span class="mi">28</span><span class="p">]]</span>

<span class="c1"># cumulative sum of explained ration</span>
<span class="n">cumulative_var_explained</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca_var_explained</span><span class="p">)</span>

<span class="c1"># cumulative distribution function for principle componets</span>
<span class="c1"># based on explained variance ratio</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_color_codes</span><span class="p">(</span><span class="s2">&quot;pastel&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">X_train_for_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">cumulative_var_explained</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_train_for_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Principal Components:  Cumulative Explained Variance Ratio&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="alt text" src="https://github.com/ERych/erych.github.io/blob/master/images/PCA.png" title="PCA Results"></p>
<p>After assessing the importance of our variables, we built our predictive models using the training dataset. Below is a summary of each model, model analysis, and model tuning performed. To assess initial model performance, we used a cross validated accuracy score.</p>
<p>Random Forest</p>
<p>Random Forest is a classification model that is built on an ensemble of decision trees. It combines multiple weak learners to build a strong learner and draws a random bootstrap sample of size n from the training data with replacement. Random Forest performs the best possible split at the nodes according to the information gain. This is repeated k number of times, and the predictions are aggregated and assigned class labels based on majority vote.</p>
<p>We chose Random Forest because our network’s input contains several categorical features. Our Random Forest model achieved the following accuracy for predicting level of interest:</p>
<p>These three accuracy scores represents a high/low estimate of accuracy, with the mean minus standard deviation, the mean, and the mean plus one standard deviation, respectively.</p>
<p>We used a Receiver Operating Characteristic curve (or ROC curve) to plot the true positive rate against the false positive rate for the different possible cutpoints of our model. Our Random Forest model performed best predicting class 1, or “medium” interest listings.</p>
<p>K Nearest Neighbors (KNN)</p>
<p>KNN is a non-parametric classification algorithm that stores all occurences and classifies new ones on the basis of similarity measurement training. We chose this lazy learner technique because our dataset is largely non-parametric, relatively non-linear and unsupervised at times. Using all numeric variables, our KNN model achieved the following accuracy for predicting level of interest:</p>
<p>These three accuracy scores represents a high/low estimate of accuracy, with the mean minus standard deviation, the mean, and the mean plus one standard deviation, respectively.</p>
<p>Using the numeric variables determined by random forest feature selection, our KNN model achieved the following accuracy for predicting level of interest:</p>
<p>We used a Receiver Operating Characteristic curve (or ROC curve) to plot the true positive rate against the false positive rate for the different possible cutpoints of our model. Like our Random Forest model, the KNN model performed best predicting class 1, or “medium” interest listings.</p>
<p>Logistic Regression
Logistic Regression is a classification algorithm containing a categorical response variable used for predictive modeling. We chose Logistic Regression because we seeked to discover the relationships between our numerous explanatory variables and the categorical response, Level of Interest, from a statistical significance perspective.</p>
<p>Using all numeric variables, our Logistic Regression model achieved the following accuracy for predicting Level of Interest:</p>
<p>These three accuracy scores represents a high/low estimate of accuracy, with the mean minus standard deviation, the mean, and the mean plus one standard deviation, respectively.</p>
<p>Using the numeric variables determined by random forest feature selection, our Logistic Regression model achieved the following accuracy for predicting level of interest:</p>
<p>We used a Receiver Operating Characteristic curve (or ROC curve) to plot the true positive rate against the false positive rate for the different possible cutpoints of our model. Like both previous models, the Logistic Regression model performed best predicting class 1, or “medium” interest listings.</p>
<p>Performance Optimization</p>
<p>Grid search was chosen for Random Forest as it is the  best classifier, according to ROC curves. The grid search provided by Scikit-learn GridSearchCV generates candidates from a “grid” of parameter values. When “fitting” it on a dataset, all the possible combinations of parameter values are evaluated and the best combination is retained. The challenge we faced was grid search’s very slow processing speeds, restricting us to use it on only some of our models. As seen below, the accuracy score was not dramatically improved using grid search.</p>
<p>Conclusion</p>
<p>Apartment hunting is challenging.  For the renter, the process of relocating, assessing housing options, and making a long-term decision on a residency can be a tiring and time consuming process.  For the landlord or broker, the task of identifying how best to showcase and price a listing can be complex and overwhelming.  Companies such as Rent Hop are trying to simplify the apartment hunting process for everyone involved using a data-driven approach.</p>
<p>Through the utilization of random forest modeling, KNN and Logistic Regression, our team was able to  successfully outline the degrees of accuracy between the various predictors of apartment demand. From the random forest perspective, the random forest classifier deemed useful through it’s ability to estimate roughly half of the available features are important in regards to decreasing node impurity. Of those features, those considered the most important in predicting apartment demand are location, price, and time. From there, KNN implementation enabled us to better understand the high/low estimates of accuracy for predicting apartment demand. The rounded intervals of (0.664, 0.669, 0.673), though not as accurate as our random forest modeling results, nonetheless provided insight into rental demand determinants. Very similar in nature to these results were those coming out of our logistic regression approach, with rounded intervals of (0.689, 0.691, 0.694). The combination of these intervals and our coefficient matrices revealed the odds of labeling interest levels based on the increase or decrease in the number of features listed in the model, supplementing our preliminary insights from random forest modeling and KNN.</p>
<p>There is further research to be study rental apartment demand using this data. First, we recommend increasing the domain knowledge of the data, for example, interviews with New York City apartment searchers or real estate professionals. Second, we recommend further feature engineering to improve accuracy. We believe that investing in the Google Maps API will be valuable to provide location data that can be used to target interest level. Finally, we believe that there is room to explore model tuning techniques, beyond grid search, to improve performance and accuracy.</p>
            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Elizabeth Rychlinski &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
    </div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>

</body>
</html>